# Streamlit Local LLM Chat (Ollama + Phi)

A simple and interactive Streamlit web interface that connects to a locally running Large Language Model (LLM) using Ollama.

This project demonstrates how to integrate a frontend UI with a locally hosted AI model without using cloud APIs.

---

## Features

- Streamlit-based web interface
- Connects to locally installed LLM via Ollama
- Conversation history panel (sidebar)
- Reset chat functionality
- Lightweight model support (Phi)
- Fully local execution (no external API calls)

---

## Tech Stack

- Python
- Streamlit
- Ollama
- Requests
- Git & GitHub

---

## Installation

### 1. Clone the repository

```bash
git clone https://github.com/SalmanSajjad95/Streamlit-Ollama-Local-LLM-Chat.git
cd Streamlit-Ollama-Local-LLM-Chat